---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%"
)
```

# SVODT: Support Vector Machine based Oblique Decision Trees

<!-- badges: start -->

<!-- badges: end -->

**SVODT** (Support Vector Machine Oblique Decision Trees) implements a hybrid classifier combining oblique decision trees with Support Vector Machines. Unlike traditional decision trees that use axis-parallel splits, SVMODT uses linear SVM hyperplanes at each node, creating oblique decision boundaries that aim to better capture complex patterns in data.

## Installation

You can install the development version of svodt from [GitHub](https://github.com/) with:

``` r
# install.packages("devtools")
devtools::install_github("AneeshAgarwala/svodt")
```

## Key Features

-   **Linear SVM splits** for simple decision boundaries

-   **Binary classification**; currently only limited to data with binary response variable

-   **Flexible feature selection** (random, mutual information, correlation)

-   **Penalized Feature selection** applies to penalty to used features at ancestor nodes for diversified feature selection

-   **Dynamic Feature Selection** allows user to either randomize or decrease the number of features in child nodes

-   **Class weight support** for imbalanced data (balanced, balanced sub-sample, custom weights)

-   **Node-specific scaling** for improved performance

## Examples

``` r
library(SVMODT)

# Load data
data(wdbc)  # Assuming you have this dataset
wdbc$diagnosis <- factor(wdbc$diagnosis)

# Split
set.seed(123)
train_idx <- sample(nrow(wdbc), 0.8 * nrow(wdbc))
train_data <- wdbc[train_idx, ]
test_data <- wdbc[-train_idx, ]
```

### SVODT Tree Workflow

``` r
# Train with class weights
tree <- svm_split(
  data = train_data,
  response = "diagnosis",
  max_depth = 4,
  max_features = 2,
  feature_method = "mutual",
  class_weights = "balanced",
  verbose = TRUE
)

# Predict
predictions <- svm_predict_tree(tree, test_data)

# Visualize - only works for trees with 2 features at each node
viz <- visualize_svm_tree(
  tree = tree,
  original_data = train_data,
  response_col = "diagnosis"
)
```

### Advanced Usage

#### Feature Selection with Penalties
```r
# Penalize previously used features to promote diversity
tree <- svm_split(
  data = train_data,
  response = "Species",
  max_depth = 4,
  max_features = 3,
  feature_method = "mutual",
  penalize_used_features = TRUE,
  feature_penalty_weight = 0.5
)
```

#### Dynamic Feature Selection
```r
# Decrease number of features at deeper levels
tree <- svm_split(
  data = train_data,
  response = "Species",
  max_depth = 5,
  max_features = 10,
  max_features_strategy = "decrease",
  max_features_decrease_rate = 0.8
)

# Random feature selection at each node
tree <- svm_split(
  data = train_data,
  response = "Species",
  max_features_strategy = "random",
  max_features_random_range = c(0.3, 0.8)
)
```

#### Handle Imbalanced Data
```r
# Balanced class weights
tree <- svm_split(
  data = train_data,
  response = "Species",
  class_weights = "balanced"
)

# Custom class weights
custom_weights <- c("setosa" = 1, "versicolor" = 2)
tree <- svm_split(
  data = train_data,
  response = "Species",
  class_weights = "custom",
  custom_class_weights = custom_weights
)
```
