% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/svmodt_info_gain.R
\name{info_gain}
\alias{info_gain}
\title{Calculate information gain of a feature}
\usage{
info_gain(feature, target, metric = c("entropy", "gini"))
}
\arguments{
\item{feature}{A vector of categorical feature values.}

\item{target}{A vector of class labels corresponding to \code{feature}.}

\item{metric}{Character; the impurity metric to use. One of \code{"entropy"} or \code{"gini"}.}
}
\value{
Numeric value of information gain: the reduction in impurity from the split.
}
\description{
Computes the information gain of splitting \code{target} using a categorical \code{feature}.
The impurity metric can be entropy or Gini impurity. Used internally for decision tree splits.
}
\details{
- Information gain is calculated as:
\deqn{IG = I(parent) - \sum_{v \in levels(feature)} (n_v / n) \cdot I(child_v)}
  where \(I(\cdot)\) is the chosen impurity metric, \(n_v\) is the number of samples
  in child node \(v\), and \(n\) is the total number of samples.
- Only levels with nonzero counts are considered to avoid numerical issues.
}
\keyword{internal}
