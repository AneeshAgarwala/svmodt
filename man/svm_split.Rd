% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/svmodt_tree.R
\name{svm_split}
\alias{svm_split}
\title{Build an Oblique Decision Tree Using SVM Splits}
\usage{
svm_split(
  data,
  response,
  depth = 1,
  max_depth = 3,
  min_samples = 5,
  max_features = NULL,
  feature_method = c("random", "mutual", "cor"),
  max_features_strategy = c("constant", "random", "decrease"),
  max_features_decrease_rate = 0.8,
  max_features_random_range = c(0.3, 1),
  penalize_used_features = FALSE,
  feature_penalty_weight = 0.5,
  used_features = character(0),
  class_weights = c("none", "balanced", "balanced_subsample", "custom"),
  custom_class_weights = NULL,
  verbose = FALSE,
  all_classes = NULL,
  ...
)
}
\arguments{
\item{data}{A data frame containing predictors and response.}

\item{response}{A string specifying the response column name in `data`.}

\item{depth}{Integer, current recursion depth (used internally).}

\item{max_depth}{Maximum depth of the tree.}

\item{min_samples}{Minimum number of samples required to split a node.}

\item{max_features}{Maximum number of features to consider at a split.}

\item{feature_method}{Feature selection method: "random", "mutual", or "cor".}

\item{max_features_strategy}{Strategy to adjust max features: "constant", "random", "decrease".}

\item{max_features_decrease_rate}{Decrease rate if `max_features_strategy = "decrease"`.}

\item{max_features_random_range}{Numeric vector, min and max fraction if `max_features_strategy = "random"`.}

\item{penalize_used_features}{Logical; if TRUE, penalize features used in ancestor nodes.}

\item{feature_penalty_weight}{Numeric weight for penalization (0–1).}

\item{used_features}{Character vector of features already used in ancestor nodes (internal use).}

\item{class_weights}{How to handle class imbalance: "none", "balanced", "balanced_subsample", "custom".}

\item{custom_class_weights}{Optional named vector of custom class weights.}

\item{verbose}{Logical; if TRUE, prints node info during recursion.}

\item{all_classes}{Optional character vector of all possible response classes (internal use).}

\item{...}{Additional arguments passed to underlying SVM fitting function.}
}
\value{
A nested list representing the decision tree. Each node contains:
\describe{
  \item{is_leaf}{Logical, TRUE if the node is a leaf.}
  \item{model}{Fitted SVM model at the node (for internal nodes).}
  \item{features}{Selected features at this node.}
  \item{scaler}{Scaling information for the node.}
  \item{left}{Left child node (decision > 0).}
  \item{right}{Right child node (decision ≤ 0).}
  \item{depth}{Depth of the node.}
  \item{n}{Number of samples at the node.}
  \item{max_features_used}{Maximum features used at this node.}
  \item{penalty_applied}{Logical, whether feature penalties were applied.}
  \item{class_weights_used}{Class weights applied at this node.}
}
}
\description{
Constructs a decision tree where each internal node uses a Support Vector
Machine (SVM) to determine the split. Supports dynamic feature selection,
feature penalization, scaling, and class weighting.
}
\details{
This function recursively splits the dataset using an SVM at each node, stopping
according to maximum depth, minimum samples, or pure nodes. Features can be
selected randomly, by mutual information, or by correlation, and previously
used features can be penalized to encourage diversity. The tree supports
scaling of numeric features and flexible class weighting schemes.
}
\examples{
data(iris)
tree <- svm_split(
  data = iris,
  response = "Species",
  max_depth = 3,
  min_samples = 5,
  feature_method = "random",
  verbose = TRUE
)

}
