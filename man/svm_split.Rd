% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/svmodt_tree.R
\name{svm_split}
\alias{svm_split}
\title{Build an Oblique Decision Tree Using SVM Splits}
\usage{
svm_split(
  data,
  response,
  depth = 1,
  max_depth = 3,
  min_samples = 5,
  max_features = NULL,
  feature_method = c("random", "mutual", "cor"),
  max_features_strategy = c("constant", "random", "decrease"),
  max_features_decrease_rate = 0.8,
  max_features_random_range = c(0.3, 1),
  penalize_used_features = FALSE,
  feature_penalty_weight = 0.5,
  used_features = character(0),
  class_weights = c("none", "balanced", "balanced_subsample", "custom"),
  custom_class_weights = NULL,
  verbose = FALSE,
  all_classes = NULL,
  ...
)
}
\arguments{
\item{data}{A data frame containing predictor variables and the response variable.}

\item{response}{A string specifying the name of the response variable (target column).}

\item{max_depth}{Maximum depth of the tree. Default is 3.}

\item{min_samples}{Minimum number of samples required to split a node. Default is 5.}

\item{max_features}{Maximum number of features to consider at each split.}

\item{feature_method}{Feature selection method: "random", "mutual", or "cor".}

\item{max_features_strategy}{Strategy for dynamic max_features: "constant", "random", or "decrease".}

\item{max_features_decrease_rate}{Rate at which max_features decreases if using "decrease".}

\item{max_features_random_range}{Range of fraction of features to sample if using "random" strategy.}

\item{penalize_used_features}{Logical; if TRUE, previously used features are penalized.}

\item{feature_penalty_weight}{Numeric; weight of the penalty applied to used features.}

\item{class_weights}{Class weighting scheme: "none", "balanced", "balanced_subsample", or "custom".}

\item{custom_class_weights}{Optional named vector of custom class weights.}

\item{verbose}{Logical; if TRUE, prints progress and node information.}

\item{...}{Additional arguments passed to the underlying SVM fitting function.}
}
\value{
A nested list representing the decision tree. Each node contains:
\describe{
  \item{is_leaf}{Logical, TRUE if the node is a leaf.}
  \item{model}{Fitted SVM model at the node (for internal nodes).}
  \item{features}{Selected features at this node.}
  \item{scaler}{Scaling information for the node.}
  \item{left}{Left child node (decision > 0).}
  \item{right}{Right child node (decision â‰¤ 0).}
  \item{depth}{Depth of the node.}
  \item{n}{Number of samples at the node.}
  \item{max_features_used}{Maximum features used at this node.}
  \item{penalty_applied}{Logical, whether feature penalties were applied.}
  \item{class_weights_used}{Class weights applied at this node.}
}
}
\description{
Constructs a decision tree where each internal node uses a Support Vector
Machine (SVM) to determine the split. Supports dynamic feature selection,
feature penalization, scaling, and class weighting.
}
\details{
This function recursively splits the dataset using an SVM at each node, stopping
according to maximum depth, minimum samples, or pure nodes. Features can be
selected randomly, by mutual information, or by correlation, and previously
used features can be penalized to encourage diversity. The tree supports
scaling of numeric features and flexible class weighting schemes.
}
\examples{
\dontrun{
data(iris)
tree <- svm_split(
  data = iris,
  response = "Species",
  max_depth = 3,
  min_samples = 5,
  feature_method = "random",
  verbose = TRUE
)
print_svm_tree(tree)
}

}
